# -*- coding: utf-8 -*-
"""
ä¼šè¨ˆä¸æ­£ãƒªã‚¹ã‚¯ã‚¹ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ãƒ„ãƒ¼ãƒ« v1.0.0
Simple Financial Fraud Detection Tool

Author: macro-observer (CPA / Financial Auditor)
Date: 2025-01-22
License: MIT
"""

from __future__ import annotations

import os
import sys
import re
import io
import math
import time
import zipfile
import logging
import asyncio
import urllib.parse
import warnings
import html
import getpass
from datetime import datetime, timedelta
from typing import List, Dict, Optional, Tuple, Any, Union

# ã‚µãƒ¼ãƒ‰ãƒ‘ãƒ¼ãƒ†ã‚£è£½ãƒ©ã‚¤ãƒ–ãƒ©ãƒª
import pandas as pd
import aiohttp
import feedparser
from bs4 import BeautifulSoup
from dotenv import load_dotenv
import nest_asyncio

# ==========================================
# åˆæœŸè¨­å®š
# ==========================================

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# ç’°å¢ƒå¤‰æ•°èª­ã¿è¾¼ã¿
load_dotenv()

# è­¦å‘Šã®æŠ‘åˆ¶
warnings.simplefilter(action='ignore', category=FutureWarning)
warnings.simplefilter(action='ignore', category=UserWarning)

# asyncioã®ã‚¤ãƒ™ãƒ³ãƒˆãƒ«ãƒ¼ãƒ—ç«¶åˆå›é¿
nest_asyncio.apply()

# ãƒ¡ã‚¿æƒ…å ±
__title__ = 'Simple Financial Fraud Detection Tool'
__version__ = '1.0.0'
__author__ = 'CPA Developer'


# ==========================================
# 1. Configã‚¯ãƒ©ã‚¹ (å®šæ•°ãƒ»è¨­å®šç®¡ç†)
# ==========================================
class Config:
    """
    ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³å…¨ä½“ã®å®šæ•°ãƒ»è¨­å®šã‚’ä¸€å…ƒç®¡ç†ã™ã‚‹ã‚¯ãƒ©ã‚¹ã€‚
    """
    # --- News API Settings ---
    NEWS_ROLES = "CFO OR æœ€é«˜è²¡å‹™è²¬ä»»è€… OR è²¡å‹™ OR çµŒç† OR å½¹å“¡"
    NEWS_ACTIONS = "è¾ä»» OR é€€ä»» OR äº¤ä»£ OR è¾è· OR æ›´è¿­ OR è§£ä»»"
    NEWS_RISK_KEYWORDS = ["æ›´è¿­", "ä¸€èº«ä¸Š", "çªç„¶", "ä¸æ­£", "å‡¦åˆ†", "ä¸æ˜æœ—", "äº¤ä»£", "ç•°å‹•"]
    NEWS_SHORT_TERM_KEYWORDS = ["çŸ­æœŸé–“", "ã‚ãšã‹", "å°±ä»»ç›´å¾Œ", "ãƒ¶æœˆ", "ã‚¹ãƒ”ãƒ¼ãƒ‰"]

    # --- JPX Settings ---
    JPX_URL = "https://www.jpx.co.jp/markets/statistics-equities/misc/01.html"

    # --- Financial / Sector Settings ---
    FINANCIAL_SECTORS = ['éŠ€è¡Œæ¥­', 'è¨¼åˆ¸ã€å•†å“å…ˆç‰©å–å¼•æ¥­', 'ä¿é™ºæ¥­', 'ãã®ä»–é‡‘èæ¥­']
    BIG4_KEYWORDS = ["ãƒˆãƒ¼ãƒãƒ„", "ã‚ãšã•", "æ–°æ—¥æœ¬", "PwC", "ï¼°ï½—ï¼£", "ã‚ã‚‰ãŸ", "Deloitte", "EY", "KPMG"]
    MANUFACTURING_SECTORS = [
        'æ°´ç”£ãƒ»è¾²æ—æ¥­', 'é‰±æ¥­', 'å»ºè¨­æ¥­', 'é£Ÿæ–™å“', 'ç¹Šç¶­è£½å“', 'ãƒ‘ãƒ«ãƒ—ãƒ»ç´™', 'åŒ–å­¦',
        'åŒ»è–¬å“', 'çŸ³æ²¹ãƒ»çŸ³ç‚­è£½å“', 'ã‚´ãƒ è£½å“', 'ã‚¬ãƒ©ã‚¹ãƒ»åœŸçŸ³è£½å“', 'é‰„é‹¼', 'éé‰„é‡‘å±',
        'é‡‘å±è£½å“', 'æ©Ÿæ¢°', 'é›»æ°—æ©Ÿå™¨', 'è¼¸é€ç”¨æ©Ÿå™¨', 'ç²¾å¯†æ©Ÿå™¨', 'ãã®ä»–è£½å“'
    ]

    # --- XBRL Parsing Priority Map ---
    # ä¼šè¨ˆåŸºæº–ã”ã¨ã®ã‚¿ã‚°åã®æºã‚‰ãã‚’å¸åã™ã‚‹ãŸã‚ã®ãƒãƒƒãƒ”ãƒ³ã‚°
    PRIORITY_MAP_SINGLE = {
        'Sales': [
            'OrdinaryIncomeSummaryOfBusinessResults',           # J-GAAP
            'RevenueIFRSSummaryOfBusinessResults',              # IFRS
            'RevenuesUSGAAPSummaryOfBusinessResults',           # US-GAAP
            'NetSalesSummaryOfBusinessResults',
            'SalesAndFinancialServicesRevenueIFRS',
            'TotalNetRevenuesIFRS', 'SalesRevenuesIFRS', 'OperatingRevenuesIFRSKeyFinancialData',
            'OrdinaryRevenue', 'OperatingRevenue1', 'Revenue', 'NetSales', 'Revenues'
        ],
        'OpIncome': [
            'OperatingProfitLossIFRSSummaryOfBusinessResults',  # IFRS
            'OperatingIncomeLossSummaryOfBusinessResults',      # US-GAAP
            'OrdinaryIncomeLossSummaryOfBusinessResults',       # J-GAAP
            'OrdinaryProfit', 'OrdinaryIncome', 'OrdinaryIncomeLoss',
            'OperatingProfit', 'OperatingIncome',
            'OperatingProfitLossIFRS', 'ProfitLossFromOperatingActivities',
            'ProfitLossBeforeTaxIFRSSummaryOfBusinessResults',
            'ProfitLossBeforeTaxUSGAAPSummaryOfBusinessResults',
            'ProfitLossBeforeTaxIFRS'
        ],
        'NetIncome': [
            'ProfitLossAttributableToOwnersOfParentIFRSSummaryOfBusinessResults',
            'NetIncomeLossAttributableToOwnersOfParentUSGAAPSummaryOfBusinessResults',
            'ProfitLossAttributableToOwnersOfParentSummaryOfBusinessResults',
            'ProfitLossAttributableToOwnersOfParent', 'NetIncome', 'ProfitLoss'
        ],
        'TotalAssets': [
            'TotalAssetsIFRSSummaryOfBusinessResults',
            'TotalAssetsUSGAAPSummaryOfBusinessResults',
            'TotalAssetsSummaryOfBusinessResults',
            'AssetsIFRS', 'Assets', 'TotalAssets'
        ],
        'NetAssets': [
            'NetAssetsSummaryOfBusinessResults', 'EquityIFRS', 'TotalEquity', 'NetAssets'
        ],
        'OpCashFlow': [
            'NetCashProvidedByUsedInOperatingActivitiesSummaryOfBusinessResults',
            'NetCashProvidedByUsedInOperatingActivities',
            'CashFlowsFromUsedInOperatingActivitiesIFRSSummaryOfBusinessResults',
            'CashFlowsFromUsedInOperatingActivitiesUSGAAPSummaryOfBusinessResults'
        ],
        'CurrentAssets': ['CurrentAssets', 'AssetsCurrent', 'CurrentAssetsIFRS'],
        'CurrentLiabilities': ['CurrentLiabilities', 'LiabilitiesCurrent'],
        'RetainedEarnings': ['RetainedEarnings', 'RetainedEarningsIFRS'],
        'CashAndEquivalents': ['CashAndCashEquivalents', 'CashAndDeposits'],
        'PPE': ['PropertyPlantAndEquipment', 'PropertyPlantAndEquipmentNet']
    }

    # --- XBRL Summation Groups ---
    # è¤‡æ•°ã®ã‚¿ã‚°ã‚’åˆç®—ã™ã‚‹å¿…è¦ãŒã‚ã‚‹é …ç›®
    XBRL_TAG_GROUPS = {
        'Receivables': [
            'AccountsReceivableTrade', 'NotesReceivableTrade', 'TradeAndOtherReceivables',
            'TradeAndOtherReceivables3CAIFRS', 'TradeAndOtherReceivablesCAIFRS',
            'TradeReceivablesOtherReceivablesAndContractAssetsCAIFRS',
            'ReceivablesRelatedToFinancialServicesCAIFRS', 'NotesAndAccountsReceivableTradeAndContractAssets',
            # IFRS/Financials
            'TradeReceivables2AssetsIFRS', 'LeaseReceivablesCA', 'AccountsReceivableInstallmentSalesCALEA',
            'OperatingLoansCA', 'LoansInCreditCardBusinessAssetsIFRS', 'LoansInBankingBusinessAssetsIFRS',
            'InstallmentLoans', 'NetInvestmentInLeases', 'LoansToCustomers', 'FinanceLeaseReceivables',
            'InvestmentInDirectFinancingLeases', 'OperatingLoans', 'LeaseInvestmentAssets',
            'Loans', 'InstallmentReceivables'
        ],
        'Inventory': [
            'Inventories', 'MerchandiseAndFinishedGoods', 'WorkInProcess',
            'InventoriesCAIFRS', 'MerchandiseCAIFRS', 'FinishedGoodsCAIFRS', 'RawMaterialsAndSuppliesCAIFRS',
            'InventoriesIFRS', 'InventoriesAssetsIFRS', 'RealEstateForSale', 'RealEstateUnderDevelopment',
            'RealEstateForSaleInProcess', 'OperationalInvestmentSecurities',
            'FinancialAssetsForTheSecuritiesBusinessAssetsIFRS',
            # ORIX / US GAAP Specific
            'RealEstateHeldForSale', 'RealEstateUnderDevelopment', 'AdvancesForRealEstate',
            'TradingSecurities', 'MarketableSecurities', 'Merchandise'
        ],
        'Payables': [
            'AccountsPayableTrade', 'NotesPayableTrade', 'TradeAndOtherPayables',
            'TradeAndOtherPayables3CLIFRS', 'TradeAndOtherPayablesCLIFRS',
            'AccountsPayableTradeLiabilitiesIFRS', 'NotesAndAccountsPayableTrade',
            'FinancialLiabilitiesForSecuritiesBusinessLiabilitiesIFRS'
        ]
    }


# ==========================================
# 2. ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£é–¢æ•°
# ==========================================
def get_config() -> tuple[str, list[str]]:
    """
    APIã‚­ãƒ¼ã¨å¯¾è±¡ä¼æ¥­ã‚³ãƒ¼ãƒ‰ã‚’å–å¾—ã™ã‚‹ã€‚
    1. ç’°å¢ƒå¤‰æ•° (.env)
    2. Google Colab Secrets (äº’æ›æ€§ç¶­æŒ)
    3. æ‰‹å…¥åŠ› (getpasså„ªå…ˆã€å¤±æ•—æ™‚ã¯input)
    """
    api_key = None

    # 1. ç’°å¢ƒå¤‰æ•° & Colab Secrets
    api_key = os.getenv("EDINET_API_KEY")
    
    if not api_key:
        try:
            from google.colab import userdata
            api_key = userdata.get('EDINET_API_KEY')
            logger.info("Google Colab Secretsã‹ã‚‰APIã‚­ãƒ¼ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸã€‚")
        except (ImportError, AttributeError, Exception):
            pass

    # 2. æ‰‹å…¥åŠ› (ä¿®æ­£: getpassã‚’è©¦è¡Œã—ã€å‹ã‚¨ãƒ©ãƒ¼æ™‚ã¯inputã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯)
    if not api_key:
        print("APIã‚­ãƒ¼ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        try:
            # æ©Ÿå¯†æ€§ä¿è­·ã®ãŸã‚ã€ã¾ãšã¯getpassã‚’è©¦è¡Œ
            raw_input = getpass.getpass('EDINET API Keyã‚’å…¥åŠ›ã—ã¦ãã ã•ã„: ')
            
            # Google Colabç­‰ã®ä¸€éƒ¨ç’°å¢ƒã§getpassãŒè¾æ›¸å‹ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’è¿”ã™ãƒã‚°ã¸ã®å¯¾ç­–
            if not isinstance(raw_input, str):
                raise ValueError("getpass returned non-string object")
                
            api_key = raw_input
            
        except (Exception, ValueError):
            # getpassãŒæ­£å¸¸ã«æ©Ÿèƒ½ã—ãªã„å ´åˆã®ã¿ã€æ¨™æº–å…¥åŠ›ã‚’ä½¿ç”¨
            print("â€»ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£å…¥åŠ›(getpass)ãŒåˆ©ç”¨ã§ããªã„ç’°å¢ƒã®ãŸã‚ã€æ¨™æº–å…¥åŠ›ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
            api_key = input('EDINET API Keyã‚’å…¥åŠ›ã—ã¦ãã ã•ã„: ')

        # å…±é€šã®ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°å‡¦ç†
        if api_key:
            api_key = str(api_key).strip().replace('"', '').replace("'", "")

    if not api_key:
        raise ValueError("API Key was not entered.")

    # ä¼æ¥­ã‚³ãƒ¼ãƒ‰ã®å–å¾—
    env_codes = os.getenv("TARGET_CODES")
    
    # Colab Secretsäº’æ›
    if not env_codes:
        try:
            from google.colab import userdata
            env_codes = userdata.get('TARGET_CODES')
        except: pass

    if env_codes:
        code_input = env_codes
        logger.info("ä¿å­˜ã•ã‚ŒãŸè¨­å®šã‹ã‚‰ä¼æ¥­ã‚³ãƒ¼ãƒ‰ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸã€‚")
    else:
        print(f"\n[{__title__} v{__version__}]")
        print("åˆ†æã—ãŸã„ä¼æ¥­ã‚³ãƒ¼ãƒ‰ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ï¼ˆæœ€å¤§10ç¤¾ã€å…¨è§’/åŠè§’ã‚¹ãƒšãƒ¼ã‚¹åŒºåˆ‡ã‚Šï¼‰")
        print(f"éŠ˜æŸ„ã‚³ãƒ¼ãƒ‰æ¤œç´¢: {Config.JPX_URL}")
        print("ä¾‹: 6758 9984 8306 (Enterã§æ±ºå®š)")
        code_input = input('Company Codes: ')

    codes = [c for c in re.split(r'[ ã€€]+', code_input.strip()) if c][:10]
    if not codes:
        raise ValueError("Company Code was not entered.")

    return api_key, codes


# ==========================================
# 3. å„ç¨®ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚¯ãƒ©ã‚¹
# ==========================================

class NewsClient:
    """Google News RSSã‚’åˆ©ç”¨ã—ã¦CFO/å½¹å“¡ã®è¾ä»»æƒ…å ±ã‚’æ¤œç´¢ã™ã‚‹ã‚¯ãƒ©ã‚¹ã€‚"""
    
    def __init__(self):
        self.base_url = "https://news.google.com/rss/search"
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }

    def check_cfo_news(self, company_name: str) -> tuple[list[dict[str, str]], str]:
        query = f'"{company_name}" ({Config.NEWS_ROLES}) ({Config.NEWS_ACTIONS})'
        params = {"q": query, "hl": "ja", "gl": "JP", "ceid": "JP:ja"}
        rss_url = f"{self.base_url}?{urllib.parse.urlencode(params)}"

        try:
            feed = feedparser.parse(rss_url, request_headers=self.headers)
            if hasattr(feed, 'status') and feed.status != 200:
                logger.warning(f"News feed status error: {feed.status} for {company_name}")
                return [], "âš ï¸ å–å¾—å¤±æ•— (æ¥ç¶šã‚¨ãƒ©ãƒ¼)"

            news_results = []
            overall_verdict = "âœ… æ­£å¸¸ (é–¢é€£ãƒ‹ãƒ¥ãƒ¼ã‚¹ãªã—)"

            for entry in feed.entries[:5]:
                title = entry.title
                status = "INFO"
                if any(k in title for k in Config.NEWS_SHORT_TERM_KEYWORDS):
                    status = "ğŸš¨ OUT (çŸ­æœŸé–“)"
                    overall_verdict = "ğŸš¨ OUT (CFO/å½¹å“¡ã®çŸ­æœŸé–“è¾ä»»ã‚ã‚Š)"
                elif any(k in title for k in Config.NEWS_RISK_KEYWORDS):
                    status = "âš ï¸ è­¦æˆ’"
                    if "OUT" not in overall_verdict: overall_verdict = "âš ï¸ è­¦æˆ’ (ä¸ç©ãªè¾ä»»)"

                news_results.append({"title": title, "date": entry.published, "status": status})
            return news_results, overall_verdict
        except Exception as e:
            logger.error(f"News API Error for {company_name}: {e}")
            return [], "âš ï¸ å–å¾—å¤±æ•— (API/Networkã‚¨ãƒ©ãƒ¼)"


class JpxClient:
    """JPXã‹ã‚‰æ¥­ç¨®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã™ã‚‹ã‚¯ãƒ©ã‚¹ã€‚"""

    def __init__(self):
        self.jpx_url = Config.JPX_URL
        self.base_host = "https://www.jpx.co.jp"
        self.sector_map = {}

    async def fetch_sector_data(self):
        print("JPXå…¬å¼ã‚µã‚¤ãƒˆã‹ã‚‰æœ€æ–°ã®æ¥­ç¨®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ä¸­...", end="")
        try:
            async with aiohttp.ClientSession() as session:
                async with session.get(self.jpx_url) as res:
                    if res.status != 200:
                        logger.warning(f"JPX access failed. Status: {res.status}")
                        return
                    text = await res.text()
            soup = BeautifulSoup(text, 'html.parser')
            link = soup.find('a', href=re.compile(r'data_j\.xls'))
            if not link:
                logger.warning("JPX excel link not found.")
                return
            
            # pandasã®Excelèª­ã¿è¾¼ã¿ã«ã¯openpyxlã‹xlrdãŒå¿…è¦ã€‚
            # HTMLã‹ã‚‰å–å¾—ã—ãŸãƒã‚¤ãƒŠãƒªãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦å‡¦ç†ã™ã‚‹ã‹ã€URLã‚’æ¸¡ã™
            excel_url = self.base_host + link['href']
            # æ³¨: å®Ÿéš›ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆå®Ÿè¡Œã§ã¯SSLè¨¼æ˜æ›¸ã‚¨ãƒ©ãƒ¼ç­‰ãŒå‡ºã‚‹å ´åˆãŒã‚ã‚‹ãŸã‚ã€pandasã§ç›´æ¥èª­ã‚€
            try:
                df = pd.read_excel(excel_url)
            except Exception:
                # å¤±æ•—æ™‚ã¯requestsç­‰ã§ãƒã‚¤ãƒŠãƒªå–å¾—ã—ã¦BytesIOçµŒç”±ãªã©ã®å®Ÿè£…ãŒå¿…è¦ã ãŒã€
                # ç°¡æ˜“åŒ–ã®ãŸã‚pandasã®æ©Ÿèƒ½ã«ä¾å­˜
                logger.warning("Failed to read Excel directly from URL.")
                return

            for _, row in df.iterrows():
                code = str(row.get('ã‚³ãƒ¼ãƒ‰', ''))[:4]
                sector = row.get('33æ¥­ç¨®åŒºåˆ†', 'ä¸æ˜')
                if code: self.sector_map[code] = sector.strip()
            print(" å®Œäº†")
        except Exception as e:
            logger.error(f"JPX fetch sector data failed: {e}")
            print(" å¤±æ•—")

    def get_sector(self, code: str) -> str:
        return self.sector_map.get(code[:4], "ä¸æ˜")


class EdinetClient:
    """EDINET API v2é€£æºã‚¯ãƒ©ã‚¹ã€‚"""

    def __init__(self, api_key: str):
        self.api_key = api_key
        self.base_url = "https://api.edinet-fsa.go.jp/api/v2"
        self.semaphore = asyncio.Semaphore(5)
        self.doc_cache = {}

    async def prefetch_metadata(self, session: aiohttp.ClientSession, target_codes: List[str]):
        print(f"\nAPIã‚¢ã‚¯ã‚»ã‚¹: ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚¹ã‚­ãƒ£ãƒ³ä¸­ï¼ˆéå»2å¹´ï¼‰...")
        dates = [datetime.now().date() - timedelta(days=i) for i in range(365 * 2)]

        async def fetch_date(date_obj):
            url = f"{self.base_url}/documents.json"
            params = {"date": date_obj.strftime("%Y-%m-%d"), "type": 2, "Subscription-Key": self.api_key}
            try:
                async with self.semaphore:
                    async with session.get(url, params=params, timeout=15) as res:
                        if res.status != 200: return
                        data = await res.json()
                        for item in data.get('results', []):
                            sec_code = str(item.get('secCode', ''))[:4]
                            if sec_code in target_codes and item.get('docTypeCode') == '120':
                                if sec_code not in self.doc_cache: self.doc_cache[sec_code] = []
                                self.doc_cache[sec_code].append(item)
            except Exception as e:
                logger.error(f"Metadata fetch failed for {date_obj}: {e}")

        tasks = [fetch_date(d) for d in dates]
        for i in range(0, len(tasks), 50):
            await asyncio.gather(*tasks[i:i+50])
            print(f"\r ã‚¹ã‚­ãƒ£ãƒ³é€²æ—: {min(i + 50, len(tasks))}/{len(tasks)} æ—¥å®Œäº†", end="")
        print("\n ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿å–å¾—å®Œäº†")

    async def get_target_document(self, session: aiohttp.ClientSession, sec_code_prefix: str) -> Optional[Dict[str, Any]]:
        docs = self.doc_cache.get(sec_code_prefix)
        if not docs: return None
        docs.sort(key=lambda x: x.get('submitDateTime', ''), reverse=True)
        return docs[0]

    async def fetch_xbrl_zip(self, session: aiohttp.ClientSession, doc_id: str) -> Optional[bytes]:
        url = f"{self.base_url}/documents/{doc_id}"
        params = {"type": 1, "Subscription-Key": self.api_key}
        try:
            async with self.semaphore:
                async with session.get(url, params=params, timeout=60) as res:
                    return await res.read() if res.status == 200 else None
        except Exception as e:
            logger.error(f"XBRL Zip download failed for {doc_id}: {e}")
            return None


# ==========================================
# 4. è§£æãƒ»åˆ†æãƒ­ã‚¸ãƒƒã‚¯
# ==========================================

class XbrlParser:
    """XBRLè§£æã‚¯ãƒ©ã‚¹ã€‚"""

    def parse_data(self, zip_bytes: bytes) -> tuple[pd.DataFrame | None, dict[str, Any] | None]:
        if not zip_bytes: return None, None
        try:
            with zipfile.ZipFile(io.BytesIO(zip_bytes)) as z:
                xbrl_files = [f for f in z.namelist() if f.endswith('.xbrl') and 'PublicDoc' in f]
                if not xbrl_files: return None, None

                priority_map_single = Config.PRIORITY_MAP_SINGLE
                df_combined = pd.DataFrame(index=list(priority_map_single.keys()) + ['Receivables', 'Inventory', 'Payables'],
                                           columns=['Current', 'Previous'], dtype='float64').fillna(0.0)

                gov_info_combined = {'Auditor': 'ä¸æ˜', 'PeriodEnd': None, 'Standard': 'Japan GAAP', 'isConsolidated': True, 'Industry': 'General',
                                     'related_party_hits': 0, 'related_party_amount': 0.0}

                # 1. Metadata Extraction
                for filename in xbrl_files:
                    with z.open(filename) as f:
                        soup = BeautifulSoup(f.read().decode('utf-8', errors='replace'), 'lxml-xml')
                        info = self._extract_dei_and_audit_info(soup)
                        if info['PeriodEnd']: gov_info_combined['PeriodEnd'] = info['PeriodEnd']
                        if info['Auditor'] != 'ä¸æ˜': gov_info_combined['Auditor'] = info['Auditor']
                        if info['Standard'] != 'Japan GAAP': gov_info_combined['Standard'] = info['Standard']
                        if gov_info_combined['PeriodEnd'] and gov_info_combined['Auditor'] != 'ä¸æ˜': break

                seen_group_tags = set()
                current_priorities = {}
                for cat in priority_map_single.keys():
                    for per in ['Current', 'Previous']:
                        current_priorities[(cat, per)] = float('inf')

                # 2. Data Extraction
                for filename in xbrl_files:
                    with z.open(filename) as f:
                        content = f.read().decode('utf-8', errors='replace')
                        soup = BeautifulSoup(content, 'lxml-xml')

                        target_date = gov_info_combined.get('PeriodEnd')
                        if not target_date:
                             target_date = self._infer_period_end(soup)
                             if target_date and not gov_info_combined['PeriodEnd']:
                                 gov_info_combined['PeriodEnd'] = target_date

                        ctx_map = self._map_contexts_strict(soup, target_date)
                        self._merge_financials(soup, ctx_map, df_combined, seen_group_tags, current_priorities)

                        gov_info_combined['related_party_hits'] += str(soup).count("é–¢é€£å½“äº‹è€…")
                        gov_info_combined['related_party_amount'] += self._extract_related_party_amounts(soup)

                return df_combined, gov_info_combined
        except Exception as e:
            logger.error(f"XBRL Parsing Error: {e}")
            return None, None

    def _infer_period_end(self, soup: BeautifulSoup) -> str | None:
        dates = []
        for ctx in soup.find_all(re.compile(r'.*context$')):
            period = ctx.find(re.compile(r'.*period$'))
            if period:
                date_tag = period.find(re.compile(r'.*(instant|endDate)$'))
                if date_tag: dates.append(date_tag.text.strip())
        return max(dates) if dates else None

    def _extract_dei_and_audit_info(self, soup: BeautifulSoup) -> Dict[str, Any]:
        info = {'Auditor': 'ä¸æ˜', 'PeriodEnd': None, 'Standard': 'Japan GAAP', 'isConsolidated': True}

        def clean_auditor_name(raw_text: str) -> str:
            if not raw_text: return 'ä¸æ˜'
            text = BeautifulSoup(raw_text, "html.parser").get_text()
            text = html.unescape(text)
            text = re.sub(r'[\s\u3000]+', ' ', text).strip()
            text = re.sub(r'(?:ç›£æŸ»æ³•äºº|ä¼šè¨ˆç›£æŸ»äºº)ã®åç§°[:ï¼š]?', '', text)
            text = re.sub(r'å½“ç¤¾ã®ç›£æŸ»å…¬èªä¼šè¨ˆå£«ç­‰ã¯[ã€,]', '', text)
            text = re.sub(r'æ¥­å‹™ã‚’åŸ·è¡Œã—ãŸå…¬èªä¼šè¨ˆå£«', '', text)
            text = re.sub(r'^[\s:ï¼š>ï¼ãƒ»ç­‰]+|[\s:ï¼š>ï¼ãƒ»]+$', '', text).strip()
            return text

        try:
            tags = soup.find_all(lambda t: 'DEI' in t.name)
            for t in tags:
                name, text = t.name, t.text.strip()
                if 'CurrentPeriodEndDate' in name: info['PeriodEnd'] = text
                if 'AccountingStandards' in name: info['Standard'] = text
                if 'WhetherConsolidatedFinancialStatementsArePrepared' in name:
                    info['isConsolidated'] = (text.lower() == 'true')
                if any(x in name for x in ['AuditFirmName', 'AuditFirmDescription', 'AuditFirm']):
                    match = re.search(r'((?:PwC|ï¼°ï½—ï¼£|EY|ï¼¥ï¼¹|æœ‰é™è²¬ä»»|ç›£æŸ»æ³•äºº|Deloitte|KPMG).*ç›£æŸ»æ³•äºº)', text, re.IGNORECASE)
                    if match: info['Auditor'] = clean_auditor_name(match.group(1))

            if info['Auditor'] == 'ä¸æ˜':
                audit_tags = soup.find_all(lambda t: any(k in t.name for k in ['NoteOnIndependentAudit', 'IndependentAuditorsReport', 'Auditor', 'AuditFirm', 'CorporateGovernance', 'Audits']))
                for tag in audit_tags:
                    text = tag.get_text()
                    if not text: continue
                    normalized_text = re.sub(r'[\s\u3000]+', ' ', text)
                    match = re.search(r'(?:ç›£æŸ»æ³•äºº|ä¼šè¨ˆç›£æŸ»äºº)ã®åç§°\s*[:ï¼š]?\s*(.*?ç›£æŸ»æ³•äºº)', normalized_text)
                    if match:
                        info['Auditor'] = clean_auditor_name(match.group(1))
                        break

                    keywords = [
                        r'æœ‰é™è²¬ä»»\s*ã‚ãšã•\s*ç›£æŸ»æ³•äºº', r'æœ‰é™è²¬ä»»\s*ç›£æŸ»æ³•äºº\s*ãƒˆãƒ¼ãƒãƒ„', r'ï¼¥ï¼¹\s*æ–°æ—¥æœ¬\s*æœ‰é™è²¬ä»»\s*ç›£æŸ»æ³•äºº',
                        r'ï¼°ï½—ï¼£\s*ï¼ªï½ï½ï½ï½\s*æœ‰é™è²¬ä»»\s*ç›£æŸ»æ³•äºº', r'ï¼°ï½—ï¼£\s*ã‚ã‚‰ãŸ\s*æœ‰é™è²¬ä»»\s*ç›£æŸ»æ³•äºº',
                        r'å¤ªé™½\s*æœ‰é™è²¬ä»»\s*ç›£æŸ»æ³•äºº', r'ä»°æ˜Ÿ\s*ç›£æŸ»æ³•äºº', r'ä¸‰å„ª\s*ç›£æŸ»æ³•äºº'
                    ]
                    for kw in keywords:
                        match = re.search(kw, normalized_text)
                        if match:
                            info['Auditor'] = clean_auditor_name(match.group(0))
                            break
                    if info['Auditor'] != 'ä¸æ˜': break

            if info['Auditor'] == 'ä¸æ˜':
                text_sample = soup.get_text()[:50000]
                match = re.search(r'((?:PwC|ï¼°ï½—ï¼£|EY|ï¼¥ï¼¹|æœ‰é™è²¬ä»»|ç›£æŸ»æ³•äºº|Deloitte|KPMG)[\s\u3000]*[^\s\u3000]+ç›£æŸ»æ³•äºº)', text_sample, re.IGNORECASE)
                if match: info['Auditor'] = clean_auditor_name(match.group(1))
        except Exception:
            pass
        return info

    def _map_contexts_strict(self, soup: BeautifulSoup, target_date_str: str | None) -> dict[str, str]:
        ctx_map = {}
        if not target_date_str: return {}
        try:
            target_dt = datetime.strptime(target_date_str, "%Y-%m-%d")
            prev_dt = target_dt.replace(year=target_dt.year - 1)
            target_dates = {target_date_str, (target_dt + timedelta(days=1)).strftime("%Y-%m-%d")}
            prev_dates = {prev_dt.strftime("%Y-%m-%d"), (prev_dt + timedelta(days=1)).strftime("%Y-%m-%d")}
        except: return {}

        contexts = soup.find_all(re.compile(r'.*context$'))
        for ctx in contexts:
            cid = ctx.get('id', '')
            is_nc = 'NonConsolidated' in cid
            if 'Separate' in cid or 'Individual' in cid: continue
            if any(x in cid for x in ['Segment', 'Row', 'Column']): continue

            has_member = ctx.find(re.compile(r'.*explicitMember$'))
            if has_member:
                member_str = str(has_member)
                if not ('ConsolidatedMember' in member_str or (is_nc and 'NonConsolidatedMember' in member_str)):
                    continue

            period = ctx.find(re.compile(r'.*period$'))
            if not period: continue
            date_tag = period.find(re.compile(r'.*(instant|endDate)$'))
            if date_tag:
                dt_text = date_tag.text.strip()
                if dt_text in target_dates: ctx_map[cid] = 'Current_NC' if is_nc else 'Current'
                elif dt_text in prev_dates: ctx_map[cid] = 'Previous_NC' if is_nc else 'Previous'
        return ctx_map

    def _merge_financials(self, soup: BeautifulSoup, ctx_map: dict, df: pd.DataFrame, seen_group: set, priorities: dict):
        priority_map_single = Config.PRIORITY_MAP_SINGLE
        for col in ['Current', 'Previous']:
            col_nc = col + '_NC'
            for cat, tag_list in priority_map_single.items():
                for idx, pattern in enumerate(tag_list):
                    if idx >= priorities[(cat, col)]: continue
                    elements = soup.find_all(lambda t: t.name and t.name.split(':')[-1] == pattern)
                    found_val = None
                    for el in elements:
                        c_type = ctx_map.get(el.get('contextRef'))
                        if c_type == col:
                            try: found_val = float(el.text.strip()); break
                            except: continue
                        elif c_type == col_nc and (found_val is None):
                            try: found_val = float(el.text.strip())
                            except: continue
                    if found_val is not None:
                        df.at[cat, col] = found_val
                        priorities[(cat, col)] = idx
                        break

            for cat, tag_list in Config.XBRL_TAG_GROUPS.items():
                for pattern in tag_list:
                    elements = soup.find_all(lambda t: t.name and t.name.split(':')[-1] == pattern)
                    val_c = 0.0
                    found_c = False
                    for el in elements:
                        if ctx_map.get(el.get('contextRef')) == col:
                            try:
                                val_c += float(el.text.strip())
                                found_c = True
                            except: continue

                    if found_c:
                        key = (pattern, col)
                        if key not in seen_group:
                            df.at[cat, col] += val_c
                            seen_group.add(key)
                    else:
                        val_nc = 0.0
                        found_nc = False
                        for el in elements:
                            if ctx_map.get(el.get('contextRef')) == col_nc:
                                try:
                                    val_nc += float(el.text.strip())
                                    found_nc = True
                                except: continue
                        if found_nc:
                            key = (pattern, col)
                            if key not in seen_group:
                                df.at[cat, col] += val_nc
                                seen_group.add(key)

    def _extract_related_party_amounts(self, soup: BeautifulSoup) -> float:
        total = 0.0
        for t in soup.find_all(lambda t: t.name and 'RelatedPartyTransactions' in t.name and 'Amount' in t.name):
            try: total += abs(float(t.text.strip()))
            except: continue
        return total


class FinancialAnalyzer:
    """è²¡å‹™æŒ‡æ¨™è¨ˆç®—ã‚¯ãƒ©ã‚¹ã€‚"""

    def __init__(self):
        self.financial_sectors = Config.FINANCIAL_SECTORS
        self.big4_keywords = Config.BIG4_KEYWORDS
        self.manufacturing_sectors = Config.MANUFACTURING_SECTORS

    def is_financial_company(self, name: str, sector: str = "") -> bool:
        if any(k in (name or "") for k in ["éŠ€è¡Œ", "è¨¼åˆ¸", "ä¿é™º", "ãƒªãƒ¼ã‚¹", "æŠ•è³‡"]): return True
        return sector in self.financial_sectors

    def check_auditor(self, auditor_name: str) -> tuple[str, str]:
        name = auditor_name or "ä¸æ˜"
        is_big4 = any(k in name for k in self.big4_keywords)
        res = "âœ… å®‰å¿ƒ (Big4/å¤§æ‰‹)" if is_big4 else "âš  æ³¨æ„ (æº–å¤§æ‰‹ãƒ»ä¸­å°)"
        return name, res

    def check_big_bath(self, df: pd.DataFrame) -> tuple[float | None, str]:
        try:
            ni, ta = df.at['NetIncome', 'Current'], df.at['TotalAssets', 'Current']
            if not ta or ta == 0: return None, "-"
            ratio = ni / ta
            verdict = "âœ… æ­£å¸¸"
            if ratio < -0.10: verdict = "âš ï¸ ãƒ“ãƒƒã‚°ãƒ»ãƒã‚¹ç–‘ã„"
            elif ratio < -0.05: verdict = "âš  èµ¤å­—"
            return ratio, verdict
        except Exception as e:
            logger.error(f"Big Bath Check Error: {e}")
            return None, "-"

    def check_related_party(self, hits: int, amount: float, sales: float) -> tuple[int, float, str]:
        verdict = "âœ… æ­£å¸¸" if hits <= 5 else "âš  æ³¨æ„" if hits <= 20 else "ğŸš¨ ç•°å¸¸"
        ratio = (amount / sales) if sales and sales > 0 else 0
        if ratio > 0.10: verdict += " (é‡‘é¡å¤§)"
        return hits, ratio, verdict

    def check_late_filing(self, period_end_str: str, submit_date_str: str) -> tuple[int | None, str]:
        if not period_end_str or not submit_date_str: return None, "-"
        try:
            p_end = datetime.strptime(period_end_str[:10], "%Y-%m-%d")
            s_date = datetime.strptime(submit_date_str[:10], "%Y-%m-%d")
            delta = (s_date - p_end).days
            verdict = "âœ… é©æ­£" if delta <= 100 else "âš ï¸ é…å»¶ç–‘ã„"
            return delta, verdict
        except Exception as e:
            logger.error(f"Late Filing Check Error: {e}")
            return None, "-"

    def calc_f_score(self, df: pd.DataFrame) -> tuple[float | None, str]:
        try:
            c, p = df['Current'], df['Previous']
            avg_assets = (c['TotalAssets'] + p['TotalAssets']) / 2
            if not avg_assets or avg_assets == 0: return None, "ãƒ‡ãƒ¼ã‚¿ä¸è¶³"

            rsst_acc = ((c['CurrentAssets'] - c['CurrentLiabilities']) - (p['CurrentAssets'] - p['CurrentLiabilities'])) / avg_assets
            ch_rec = (c['Receivables'] - p['Receivables']) / avg_assets
            ch_inv = (c['Inventory'] - p['Inventory']) / avg_assets

            pred = -7.893 + 0.79*rsst_acc + 2.518*ch_rec + 1.191*ch_inv
            prob = 1 / (1 + math.exp(-pred))
            verdict = "âš ï¸ é«˜ãƒªã‚¹ã‚¯" if prob > 0.01 else "âœ… ä½ãƒªã‚¹ã‚¯"
            return prob, f"{prob:.4%} ({verdict})"
        except Exception as e:
            logger.error(f"F-Score Calculation Error: {e}")
            return None, "è¨ˆç®—ã‚¨ãƒ©ãƒ¼"

    def calc_sloan_ratio(self, df: pd.DataFrame, sector: str = "ä¸æ˜") -> tuple[float | None, str]:
        try:
            ni, ocf, ta = df.at['NetIncome', 'Current'], df.at['OpCashFlow', 'Current'], df.at['TotalAssets', 'Current']
            if not ta or ta == 0: return None, "ãƒ‡ãƒ¼ã‚¿ä¸è¶³"
            ratio = (ni - ocf) / ta

            threshold = 0.25 if sector in ["æƒ…å ±ãƒ»é€šä¿¡æ¥­", "ã‚µãƒ¼ãƒ“ã‚¹æ¥­"] else 0.10
            verdict = "âœ… é©æ­£"
            if abs(ratio) > threshold: verdict = "âš  æ³¨æ„"
            return ratio, f"{ratio:.2%} -> {verdict} (åŸºæº–: Â±{threshold:.0%})"
        except Exception as e:
            logger.error(f"Sloan Ratio Calculation Error: {e}")
            return None, "-"

    def calc_turnover(self, df: pd.DataFrame) -> dict[str, dict[str, Any]] | None:
        try:
            c, p = df['Current'], df['Previous']
            if not c['Sales'] or c['Sales'] == 0: return None

            res = {}
            for item, label in [('Receivables', 'Rec'), ('Inventory', 'Inv'), ('Payables', 'Pay')]:
                if c[item] and c[item] > 0:
                    tc = (c[item]/c['Sales'])*12
                    has_prev = False
                    diff = None
                    if p[item] and p['Sales']:
                        tp = (p[item]/p['Sales'])*12
                        diff = tc - tp
                        has_prev = True

                    verdict = "âœ… é©æ­£"
                    if diff and diff > 1.0:
                        if item == 'Receivables': verdict = "âš ï¸ é•·æœŸåŒ–"
                        elif item == 'Inventory': verdict = "âš ï¸ éå‰°åœ¨åº«"
                        elif item == 'Payables': verdict = "âš ï¸ æ”¯æ‰•é…å»¶"

                    res[label] = {'val': tc, 'diff': diff, 'verdict': verdict, 'has_prev': has_prev}
                else:
                    res[label] = None
            return res
        except Exception as e:
            logger.error(f"Turnover Calculation Error: {e}")
            return None

    def calc_z_score(self, df: pd.DataFrame, name: str, sector: str) -> tuple[float | None, str]:
        if self.is_financial_company(name, sector): return None, "â„¹ï¸ å‚è€ƒå€¤ (é‡‘èæ¥­ã®ãŸã‚é©ç”¨å¤–)"
        try:
            c = df['Current']
            ta = c['TotalAssets']
            if not ta or ta == 0: return None, "ãƒ‡ãƒ¼ã‚¿ä¸è¶³"
            x1 = (c['CurrentAssets'] - c['CurrentLiabilities']) / ta
            x2 = (c['RetainedEarnings'] / ta) if c['RetainedEarnings'] else 0
            x3 = c['OpIncome'] / ta
            x4 = c['NetAssets'] / max(1, ta - c['NetAssets'])

            if sector in self.manufacturing_sectors:
                z = 1.2*x1 + 1.4*x2 + 3.3*x3 + 0.6*x4 + 1.0*(c['Sales']/ta)
                if z < 1.23: verdict = "âš ï¸ å±é™ºåŸŸ"
                elif z < 2.90: verdict = "âš  è¦æ³¨æ„ (ã‚°ãƒ¬ãƒ¼ã‚¾ãƒ¼ãƒ³)"
                else: verdict = "âœ… å¥å…¨"
            else:
                z = 6.56*x1 + 3.26*x2 + 6.72*x3 + 1.05*x4
                verdict = "âš ï¸ å±é™ºåŸŸ" if z < 1.1 else "âœ… å¥å…¨"

            return z, f"{z:.2f} ({verdict})"
        except Exception as e:
            logger.error(f"Z-Score Calculation Error: {e}")
            return None, "-"


# ==========================================
# 5. ãƒ¡ã‚¤ãƒ³å‡¦ç†ãƒ»å®Ÿè¡Œã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆ
# ==========================================

async def process_company(session, code, client, parser, analyzer, news_client, jpx):
    """1ç¤¾åˆ†ã®åˆ†æå‡¦ç†ã‚’å®Ÿè¡Œã—ã€ãƒ¬ãƒãƒ¼ãƒˆã‚’å‡ºåŠ›ã™ã‚‹ãƒ¡ã‚¤ãƒ³ãƒ­ã‚¸ãƒƒã‚¯ã€‚"""
    doc = await client.get_target_document(session, code)
    if not doc:
        logger.warning(f"No document found for {code}")
        return
    
    name = doc.get('filerName', 'Unknown')
    sector = jpx.get_sector(code)
    submit_date = doc.get('submitDateTime', '')[:10]

    # ä¸¦è¡Œã—ã¦ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’å–å¾—
    news_res, news_verdict = news_client.check_cfo_news(name)

    # XBRLãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã¨è§£æ
    zip_bytes = await client.fetch_xbrl_zip(session, doc['docID'])
    df_data, gov_info = parser.parse_data(zip_bytes)
    if df_data is None:
        logger.warning(f"Failed to parse XBRL for {code}")
        return

    # --- ãƒ¬ãƒãƒ¼ãƒˆå‡ºåŠ› ---
    print("\n" + "="*80)
    print(f"ã€åˆ†æãƒ¬ãƒãƒ¼ãƒˆã€‘ {code} {name} (æ¥­ç¨®: {sector})")
    print("="*80)
    print(f"\nã€å–å¾—ãƒ‡ãƒ¼ã‚¿ç¢ºèª (å˜ä½: ç™¾ä¸‡å††) (åŸºæº–: {gov_info.get('Standard')})ã€‘")
    print(f"{'':<15} {'Current':>15} {'Previous':>15}")
    for row in ['Sales', 'OpIncome', 'NetIncome', 'OpCashFlow', 'TotalAssets', 'Receivables', 'Inventory', 'Payables']:
        val_c = df_data.at[row, 'Current']
        val_p = df_data.at[row, 'Previous']
        c_disp = f"{val_c/1e6:,.0f}" if val_c else "-"
        p_disp = f"{val_p/1e6:,.0f}" if val_p else "-"

        label = "Op/Ord Income" if row == "OpIncome" else row
        print(f"{label:<15} {c_disp:>15} {p_disp:>15}")
    print("-" * 50)

    # I. ã‚¬ãƒãƒŠãƒ³ã‚¹ãƒ»å®šæ€§ãƒªã‚¹ã‚¯åˆ†æ
    print("\nã€I. ã‚¬ãƒãƒŠãƒ³ã‚¹ãƒ»å®šæ€§ãƒªã‚¹ã‚¯åˆ†æã€‘")
    print(f"[1] è¾ä»»ãƒ‹ãƒ¥ãƒ¼ã‚¹ç›£è¦–: {news_verdict}")
    if news_res:
        for n in news_res: print(f"      - [{n['status']}] {n['title']} ({n['date'][:10]})")

    aud_name, aud_res = analyzer.check_auditor(gov_info['Auditor'])
    print(f"\n[2] ç›£æŸ»æ³•äºº: {aud_name} -> {aud_res}")
    print("    ã€åˆ¤å®šåŸºæº–ã€‘Big4(ãƒˆãƒ¼ãƒãƒ„/ã‚ãšã•/EY/PwC)ã‚’å«ã‚€å¤§æ‰‹ç›£æŸ»æ³•äººã‹å¦ã‹ã€‚")

    bb_val, bb_res = analyzer.check_big_bath(df_data)
    bb_disp = f"{bb_val:.2%}" if bb_val is not None else "-"
    print(f"\n[3] ãƒ“ãƒƒã‚°ãƒ»ãƒã‚¹: {bb_disp} -> {bb_res}")
    print("    ã€åˆ¤å®šåŸºæº–ã€‘ç·è³‡ç”£å½“æœŸç´”åˆ©ç›Šç‡(ROA)ãŒ-10%æœªæº€ã®å·¨é¡èµ¤å­—ã€‚")

    rp_hits, rp_ratio, rp_res = analyzer.check_related_party(
        int(gov_info['related_party_hits']), 
        gov_info['related_party_amount'], 
        df_data.at['Sales', 'Current']
    )
    print(f"\n[4] é–¢é€£å½“äº‹è€…åˆ†æ\n    è¨€åŠæ•°: {rp_hits}å› / å–å¼•é¡æ¯”: {rp_ratio:.2%}\n    åˆ¤å®š: {rp_res}")
    print("    ã€åˆ¤å®šåŸºæº–ã€‘30å›ä»¥ä¸Šã€ã¾ãŸã¯å£²ä¸Šå¯¾æ¯”10%è¶…ã§ç•°å¸¸å€¤ã€‚")

    days, late_res = analyzer.check_late_filing(gov_info['PeriodEnd'], submit_date)
    print(f"\n[5] æå‡ºé…å»¶: æ±ºç®—ã‹ã‚‰{days}æ—¥çµŒé -> {late_res}")
    print("    ã€åˆ¤å®šåŸºæº–ã€‘æ±ºç®—æ—¥ã‹ã‚‰æå‡ºæ—¥ã¾ã§100æ—¥è¶…ï¼ˆé€šå¸¸ã¯90æ—¥ä»¥å†…ï¼‰ã€‚")

    # II. è²¡å‹™æ•°å€¤ãƒ»å®šé‡ãƒªã‚¹ã‚¯åˆ†æ
    print("\nã€II. è²¡å‹™æ•°å€¤ãƒ»å®šé‡ãƒªã‚¹ã‚¯åˆ†æ (æ™‚ç³»åˆ—æ¯”è¼ƒ)ã€‘")
    is_fin = analyzer.is_financial_company(name, sector)
    f_prob, f_res = analyzer.calc_f_score(df_data)
    print(f"[1] Dechow F-Score: {f_res}")
    if is_fin: print("    â„¹ï¸ ã€å‚è€ƒã€‘é‡‘èäº‹æ¥­ãŒå«ã¾ã‚Œã‚‹ãŸã‚å‚è€ƒå€¤ã§ã™ã€‚")

    s_val, s_res = analyzer.calc_sloan_ratio(df_data, sector)
    print(f"\n[2] ã‚¹ãƒ­ãƒ¼ãƒ³ãƒ»ãƒ¬ã‚·ã‚ª: {s_res}")

    print(f"\n[3] å›è»¢æœŸé–“åˆ†æ")
    turns = analyzer.calc_turnover(df_data)
    if turns:
        label_map = {'Rec': 'å£²ä¸Šå‚µæ¨©', 'Inv': 'æ£šå¸è³‡ç”£', 'Pay': 'ä»•å…¥å‚µå‹™'}
        for k, v in turns.items():
            label = label_map.get(k, k)
            if v:
                diff_str = f"{v['diff']:+.1f}" if v['has_prev'] and v['diff'] is not None else "-"
                print(f"    {label}: {v['val']:.1f}ãƒ¶æœˆ (å‰å·® {diff_str}) -> {v['verdict']}")
            else:
                print(f"    {label}: - (ãƒ‡ãƒ¼ã‚¿ãªã—)")
        if is_fin:
            print("    â„¹ï¸ ã€æ³¨é‡ˆã€‘é‡‘èãƒ»ãƒªãƒ¼ã‚¹æ¥­ã®ãŸã‚å‚è€ƒå€¤ã§ã™ã€‚")
    else:
        print("    (å£²ä¸Šé«˜ãƒ‡ãƒ¼ã‚¿ãªã—ã®ãŸã‚è¨ˆç®—ä¸å¯)")

    z_val, z_res = analyzer.calc_z_score(df_data, name, sector)
    print(f"\n[4] ã‚¢ãƒ«ãƒˆãƒãƒ³Zã‚¹ã‚³ã‚¢: {z_res}")
    print("=" * 80)


async def main_async():
    """éåŒæœŸãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    try:
        api_key, codes = get_config()
    except ValueError as e:
        logger.error(str(e))
        return

    jpx = JpxClient()
    await jpx.fetch_sector_data()
    
    client = EdinetClient(api_key)
    parser, analyzer, news = XbrlParser(), FinancialAnalyzer(), NewsClient()
    
    async with aiohttp.ClientSession() as session:
        await client.prefetch_metadata(session, codes)
        tasks = [process_company(session, c, client, parser, analyzer, news, jpx) for c in codes]
        await asyncio.gather(*tasks)


def main():
    """ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆ"""
    try:
        asyncio.run(main_async())
    except KeyboardInterrupt:
        print("\nå‡¦ç†ã‚’ä¸­æ–­ã—ã¾ã—ãŸ")
    except Exception as e:
        logger.error(f"äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")

if __name__ == "__main__":
    main()
